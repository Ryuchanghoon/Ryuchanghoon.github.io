---
layout: single
title: RNN 간단 구성
---


오랜만에 하는 포스팅입니다.
<br>
오늘은 딥러닝 공부를 하다가 재밌는걸 보게 되어서 기록을 남기고자 합니다.
<br>
<br>
살펴볼 내용은 RNN(순환 신경망)입니다.
<br>
물론 더 진보된 형태인 LSTM이 있는데, 이건 다음 포스팅에서 RNN이론과 함께 다루도록 하겠습니다.
<br>
오늘은 코드만 좀 보려고 합니다.


```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
```

필요한 모듈을 불러옵니다. Pytorch로 다룰거여서, torch내에 있는 것을 갖고 옵니다.


```python
n_hidden = 35
lr = 0.01
epochs = 1000

string = "hello changhoon what a nice day"
chars =  "abcdefghijklmnopqrstuvwxyz ?!.,:;01"
```

먼저 하이퍼파라미터 값을 지정해줍니다. 은닉층은 35, 학습률은 0.01, 에포크값은 1000으로 지정해줍니다.
<br>
문자열은 그냥 임의로 아무거나 지정했습니다.


```python
char_list = [i for i in chars]
n_letters = len(char_list)
```

위의 문자길이 만큼(chars)만큼 반복하고, 이걸 리스트로 저장합니다.
<br>
그리고 이 리스트 길이를 n_letters로 지정합니다.


```python
print(n_letters)
```

    35
    

n_letters길이를 보아하니, 35라고 나옵니다. 총 35개의 문자로 구성되어 있기 때문입니다.


```python
# 원핫 벡터
def string_to_onehot(string):
    start = np.zeros(shape = n_letters, dtype = int)
    end = np.zeros(shape = n_letters, dtype = int)

    start[-2] = 1
    end[-1] = 1
        #이렇게 해주는 이유가, 전에꺼를 다음꺼로 넘겨주기 위함.
    for i in string:
        idx = char_list.index(i)
        zero = np.zeros(shape = n_letters, dtype = int)
        
        zero[idx] = 1
        start = np.vstack([start, zero])

    output = np.vstack([start, end])
    return output
```

먼저 문자를 컴퓨터가 잘 알아들을 수 있는 숫자값으로 변환해주어야 합니다.
<br>
방식은 0으로 구성된 배열을 생성해주고, 그 문자열에 해당하는 값만 1로 변환해 줍니다.


```python
#위에 원핫 만들어준거 다시 문자로 바꾸는 함수 생성.
def onehot_to_word(onehot_1):
    onehot = torch.Tensor.numpy(onehot_1)
    return char_list[onehot.argmax()]
```

그리고 위에서 만들어준 값을 다시 문자로 바꿔야 합니다. 결과(문자)를 우리 눈으로 직접 확인하려면, 당연한거겠죠?
<br>
그래서 이것을 함수로 구현을 해줍니다.
<br>
방식을 보아하면, return값에 argmax()함수가 보이는데, 이것은 최댓값을 뽑아내는 함수 입니다. 0,1값으로 바꾼것에서 최댓값은 1이기 때문에, 그 문자가 무엇인지 바로 확인할 수 있습니다.


```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()

        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i20 = nn.Linear(input_size + hidden_size, output_size)
        self.act_fn = nn.Tanh()
    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1) #입력, 은닉층을 합치고,
        hidden = self.act_fn(self.i2h(combined)) #이걸 활성화 함수 통과,
                                                 
        output = self.i20(combined) # 결과값 계산하게끔.
        return output, hidden

        #은닉층 초기화.
    def init_hidden(self):
        return torch.zeros(1, self.hidden_size)

rnn = RNN(n_letters, n_hidden, n_letters)
```

이제 신경망 구성 입니다. RNN은 hidden layer(은닉층)가 존재하는데, 그 코드가 들어가 있습니다.
<br>
좀 더 자세한건 다음 포스팅에서 이론과 함께 보도록 하겠습니다.
<br>
그리고 참 고맙게도 이 모든 계산을 한번에 해주는 함수가 내장되어 있습니다. 맨 밑에 RNN이 그 함수 입니다.
<br>
일단은 RNN구성이 이렇게 된다 정도만 보고 넘어가겠습니다. 


```python
loss_fn = nn.MSELoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr = lr)
```

그리고 손실함수로 MSE, optimizer(최적화 함수)로 Adam을 사용했습니다. learning rate는 위에서 설정해준 0.01입니다.


```python
one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())

for i in range(epochs):
    optimizer.zero_grad()
    hidden = rnn.init_hidden()

    total_loss = 0

    for j in range(one_hot.size()[0] -1): #맨 마지막 글자는 넣을 필요 x 그래서 -1
        input_ = one_hot[j:j+1, :]
        target = one_hot[j+1]
        output, hidden = rnn.forward(input_, hidden)

        loss = loss_fn(output.view(-1), target.view(-1))
        total_loss += loss

    total_loss.backward()
    optimizer.step()

    if i % 10 == 0:
        print(total_loss)
```

    tensor(1.2575, grad_fn=<AddBackward0>)
    tensor(0.4337, grad_fn=<AddBackward0>)
    tensor(0.1702, grad_fn=<AddBackward0>)
    tensor(0.0607, grad_fn=<AddBackward0>)
    tensor(0.0224, grad_fn=<AddBackward0>)
    tensor(0.0097, grad_fn=<AddBackward0>)
    tensor(0.0033, grad_fn=<AddBackward0>)
    tensor(0.0012, grad_fn=<AddBackward0>)
    tensor(0.0006, grad_fn=<AddBackward0>)
    tensor(0.0002, grad_fn=<AddBackward0>)
    tensor(6.0099e-05, grad_fn=<AddBackward0>)
    tensor(3.1081e-05, grad_fn=<AddBackward0>)
    tensor(5.3220e-06, grad_fn=<AddBackward0>)
    tensor(4.0210e-06, grad_fn=<AddBackward0>)
    tensor(1.5523e-06, grad_fn=<AddBackward0>)
    tensor(3.0156e-07, grad_fn=<AddBackward0>)
    tensor(1.8422e-07, grad_fn=<AddBackward0>)
    tensor(4.0620e-07, grad_fn=<AddBackward0>)
    tensor(5.8717e-05, grad_fn=<AddBackward0>)
    tensor(0.0004, grad_fn=<AddBackward0>)
    tensor(1.8982e-05, grad_fn=<AddBackward0>)
    tensor(1.2023e-05, grad_fn=<AddBackward0>)
    tensor(8.9842e-06, grad_fn=<AddBackward0>)
    tensor(3.2853e-06, grad_fn=<AddBackward0>)
    tensor(8.3661e-07, grad_fn=<AddBackward0>)
    tensor(6.8516e-07, grad_fn=<AddBackward0>)
    tensor(5.2689e-06, grad_fn=<AddBackward0>)
    tensor(0.0005, grad_fn=<AddBackward0>)
    tensor(6.3499e-05, grad_fn=<AddBackward0>)
    tensor(5.9747e-06, grad_fn=<AddBackward0>)
    tensor(5.1239e-06, grad_fn=<AddBackward0>)
    tensor(3.1230e-06, grad_fn=<AddBackward0>)
    tensor(1.0381e-06, grad_fn=<AddBackward0>)
    tensor(4.2215e-07, grad_fn=<AddBackward0>)
    tensor(1.5688e-07, grad_fn=<AddBackward0>)
    tensor(3.8961e-08, grad_fn=<AddBackward0>)
    tensor(1.3526e-08, grad_fn=<AddBackward0>)
    tensor(5.8640e-09, grad_fn=<AddBackward0>)
    tensor(2.4858e-09, grad_fn=<AddBackward0>)
    tensor(3.1654e-09, grad_fn=<AddBackward0>)
    tensor(1.8571e-07, grad_fn=<AddBackward0>)
    tensor(9.0373e-05, grad_fn=<AddBackward0>)
    tensor(0.0003, grad_fn=<AddBackward0>)
    tensor(4.8961e-05, grad_fn=<AddBackward0>)
    tensor(3.0388e-05, grad_fn=<AddBackward0>)
    tensor(6.3519e-06, grad_fn=<AddBackward0>)
    tensor(1.4026e-06, grad_fn=<AddBackward0>)
    tensor(1.1158e-06, grad_fn=<AddBackward0>)
    tensor(3.2222e-07, grad_fn=<AddBackward0>)
    tensor(8.6871e-07, grad_fn=<AddBackward0>)
    tensor(5.0771e-05, grad_fn=<AddBackward0>)
    tensor(4.4149e-05, grad_fn=<AddBackward0>)
    tensor(2.7161e-05, grad_fn=<AddBackward0>)
    tensor(1.8716e-05, grad_fn=<AddBackward0>)
    tensor(3.8883e-06, grad_fn=<AddBackward0>)
    tensor(2.0652e-06, grad_fn=<AddBackward0>)
    tensor(1.0063e-05, grad_fn=<AddBackward0>)
    tensor(0.0003, grad_fn=<AddBackward0>)
    tensor(9.7276e-06, grad_fn=<AddBackward0>)
    tensor(1.5106e-05, grad_fn=<AddBackward0>)
    tensor(4.8902e-06, grad_fn=<AddBackward0>)
    tensor(1.3744e-06, grad_fn=<AddBackward0>)
    tensor(2.0329e-06, grad_fn=<AddBackward0>)
    tensor(1.3373e-05, grad_fn=<AddBackward0>)
    tensor(0.0002, grad_fn=<AddBackward0>)
    tensor(4.9924e-06, grad_fn=<AddBackward0>)
    tensor(4.5209e-06, grad_fn=<AddBackward0>)
    tensor(3.7715e-06, grad_fn=<AddBackward0>)
    tensor(1.2596e-06, grad_fn=<AddBackward0>)
    tensor(2.1641e-07, grad_fn=<AddBackward0>)
    tensor(1.6506e-07, grad_fn=<AddBackward0>)
    tensor(8.6806e-07, grad_fn=<AddBackward0>)
    tensor(7.7218e-05, grad_fn=<AddBackward0>)
    tensor(0.0001, grad_fn=<AddBackward0>)
    tensor(4.6524e-05, grad_fn=<AddBackward0>)
    tensor(1.3041e-05, grad_fn=<AddBackward0>)
    tensor(5.2785e-06, grad_fn=<AddBackward0>)
    tensor(1.5810e-06, grad_fn=<AddBackward0>)
    tensor(7.2158e-07, grad_fn=<AddBackward0>)
    tensor(5.8798e-07, grad_fn=<AddBackward0>)
    tensor(1.0210e-06, grad_fn=<AddBackward0>)
    tensor(4.1933e-05, grad_fn=<AddBackward0>)
    tensor(0.0003, grad_fn=<AddBackward0>)
    tensor(9.2447e-05, grad_fn=<AddBackward0>)
    tensor(2.5179e-05, grad_fn=<AddBackward0>)
    tensor(5.8676e-06, grad_fn=<AddBackward0>)
    tensor(1.6301e-06, grad_fn=<AddBackward0>)
    tensor(7.7389e-07, grad_fn=<AddBackward0>)
    tensor(1.0165e-06, grad_fn=<AddBackward0>)
    tensor(6.2706e-05, grad_fn=<AddBackward0>)
    tensor(3.1370e-05, grad_fn=<AddBackward0>)
    tensor(3.4762e-05, grad_fn=<AddBackward0>)
    tensor(2.2183e-05, grad_fn=<AddBackward0>)
    tensor(4.6873e-06, grad_fn=<AddBackward0>)
    tensor(2.3002e-06, grad_fn=<AddBackward0>)
    tensor(8.0863e-07, grad_fn=<AddBackward0>)
    tensor(2.0595e-07, grad_fn=<AddBackward0>)
    tensor(1.6919e-07, grad_fn=<AddBackward0>)
    tensor(1.5241e-08, grad_fn=<AddBackward0>)
    tensor(1.3632e-08, grad_fn=<AddBackward0>)
    

이제 학습을 해서 손실값이 얼마나 줄어드는지 확인하겠습니다. 처음 1.2662에서 시작해서, 0.00000088395(거의 0)까지 줄어든 것을 확인할 수 있습니다.
<br>
물론 손실값이 극단적으로 0에 수렴하는 이유도, 초기 string값이 35밖에 안되기 때문입니다.


```python
start = torch.zeros(1, n_letters)
start[:, -2] = 1 # 마지막 문자 다음에 오는 문자를 시작 문자로 지정하려고.

with torch.no_grad():
    hidden = rnn.init_hidden()
    input_ = start #첫 입력으로 start 전달.
    output_string = "" #나오는 문자 계속 붙여주기.

    for i in range(len(string)):
        output, hidden = rnn.forward(input_, hidden)
        output_string += onehot_to_word(output.data) #위에꺼 누적.
        input_ = output #이번 결과값, 다음 입력값으로.

print(output_string)
```

    hello changhoon what a nice day
    

놀랍게도 테스트 값이 초기 설정값과 동일하게 나왔습니다.
<br>
물론 위의 손실값을 보고, 똑같이 나오겠구나 예상은 됩니다.
<br>
한가지 신통방통한 점은 띄어쓰기가 5번 반복되는데, 이것 또한 잘 캐치를 했습니다.

다음 포스팅에서는 좀더 긴 문자열을 가지고 돌려봐야겠습니다.
